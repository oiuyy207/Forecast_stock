{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd705b4c-7eee-4281-bd96-80479ceef1b3",
   "metadata": {},
   "source": [
    "기본값 : 0  \n",
    "INFO 로그 필터링 : 1  \n",
    "WARNING 로그 필터링 : 2  \n",
    "ERROR 로그 필터링 : 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a1e93-46ba-47a8-ade9-ab1765392b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bc948-2cc9-4ee7-ac04-68045adfbcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98476b13-a788-49b4-8124-84128d2f34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model_layer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145deef8-0acf-4740-bb7c-9c50092d597e",
   "metadata": {},
   "source": [
    "# 데이터 셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3a4b0-3814-49b9-9692-2eb3c4b47d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M6_Universe = pd.read_csv('M6_Universe.csv')\n",
    "\n",
    "start_date = \"2021-05-15\" #OGN에셋 길이에 맞춤\n",
    "end_date = \"2022-08-20\"\n",
    "\n",
    "os.makedirs(\"./stock/asset\",exist_ok=True)\n",
    "\n",
    "for asset in tqdm(M6_Universe.symbol):\n",
    "    stock = yf.download(asset, start=start_date, end = end_date, progress = False)\n",
    "    stock.to_csv(\"./stock/asset/\"+asset+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b729ea-dc9b-4ad9-a9ac-4ab8e9261c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./stock/index\",exist_ok=True)\n",
    "\n",
    "#S&P500 / 나스닥 / 다우존스 / 미국채 10년물 금리 / 변동성 지수(S&P 500 지수 옵션에 기반한 변동성) / 골드만삭스 원자재 지수(선물옵션 기반) / 원유\n",
    "Index_list = [\"SP500\", \"NASDAQ\", \"DOWJONES\", \"TNX\", \"VIX\", \"GSCI\" , \"Oil\"]\n",
    "Index_code = ['ES=F', 'YM=F', 'NQ=F', '^TNX', '^VIX', 'GD=F', 'CL=F']\n",
    "\n",
    "Index_pd = pd.DataFrame([Index_list, Index_code],index=['name','symbol']).transpose()\n",
    "\n",
    "for i in tqdm(range(Index_pd.shape[0])):\n",
    "    stock = yf.download(Index_pd.symbol[i], start=start_date, end = end_date, progress = False)\n",
    "    stock.to_csv(\"./stock/index/\"+Index_pd.name[i]+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af01eb-a4c4-4e5b-997f-5bf6303fb3d9",
   "metadata": {},
   "source": [
    "## 데이터셋 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eea592-ca6c-4372-8588-eb32488965dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_for_transformer:\n",
    "    '''\n",
    "    [22-08-19(화)]\n",
    "    데이터셋 만들기(Ver.2)\n",
    "    \n",
    "    MI-LSTM처럼 상관 계수 구할 예정!\n",
    "    에셋별 scaler를 수동 적용.\n",
    "    Date_Range 생성.\n",
    "    원본 유지.\n",
    "    \n",
    "    메모:\n",
    "        * cal_corr / load_file / get_dataset 만들 예정\n",
    "        * 인덱스도 corr을 계산하는게 좋을까, 아니면 따로 넣는게 좋을까?\n",
    "        * 가격 그대로를 corr계산할까, 수익률[(현재-과거)/과거]로 계산할까?\n",
    "        * corr계산의 길이는? window사이즈의 2~2.5배? 아니면 고정길이?\n",
    "    \n",
    "    Parameter:\n",
    "        asset_list        : 사용 에셋 리스트. csv파일이 존재하는 에셋 이름으로 가지고 있어야함.\n",
    "        index_list        : 사용 인덱스 리스트. csv파일이 존재하는 인덱스 이름으로 가지고 있어야함.\n",
    "        pos_corr          : 양의 상관 계수의 주식 수.\n",
    "        neg_corr          : 음의 상관 계수의 주식 수.\n",
    "        path(\"./stock\")   : 에셋, 인덱스 파일 경로. asset은 path+/asset에, index는 path+/index에 있어야 함.\n",
    "        train_ratio(None) : trainset의 비율. None이면 test셋 만들지 않음.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, asset_list, index_list, pos_corr, neg_corr, path = \"./stock\"):\n",
    "        self.asset_list = asset_list\n",
    "        self.index_list = index_list\n",
    "        self.pos_corr = pos_corr\n",
    "        self.neg_corr = neg_corr\n",
    "        self.__file_path = path\n",
    "        \n",
    "        self.dataset_asset = self.load_file(self.asset_list,\"/asset\")\n",
    "        self.dataset_index = self.load_file(self.index_list,\"/index\")\n",
    "        \n",
    "        self.dataset_asset_scaled = self.dataset_asset.copy()\n",
    "        self.dataset_index_scaled = self.dataset_index.copy()\n",
    "    \n",
    "    def calculate_correlation(self,dataset,window_size):\n",
    "        #get_dataset안에서 사용될 예정\n",
    "        #에셋 이름을 받을 것이고, 길이는 이 함수 전에 미리 자를 것(window_size의 1.5배 정도)\n",
    "        #dataset은 (corr계산길이 * 100)인 pd.DataFrame\n",
    "        corr_dataset = dataset.corr(method='pearson').copy()\n",
    "        cut_dataset = dataset.iloc[-window_size:,:].copy()\n",
    "        \n",
    "        hist_idxs = [i for i in range(0,self.pos_corr+1)]#target+양의 corr 개수\n",
    "        hist_idxs.extend([i for i in range(-self.neg_corr,0,1)])#음의 corr 개수\n",
    "        \n",
    "        target_np = []\n",
    "        pos_np = []\n",
    "        neg_np = []\n",
    "        for i in range(100):\n",
    "            select_asset = corr_dataset.iloc[:,i].sort_values(ascending=False).index[hist_idxs].to_list()\n",
    "            \n",
    "            target_np.append(cut_dataset[select_asset[0]].to_numpy()[np.newaxis,:])\n",
    "            pos_np.append(cut_dataset[select_asset[1:self.pos_corr+1]].to_numpy().T[np.newaxis,:])\n",
    "            neg_np.append(cut_dataset[select_asset[-self.neg_corr:]].to_numpy().T[np.newaxis,:])\n",
    "        \n",
    "        target_np = np.concatenate(target_np,axis=0) #(100, window_size)\n",
    "        pos_np = np.concatenate(pos_np,axis=0) #(100, self.pos_corr, window_size)\n",
    "        neg_np = np.concatenate(neg_np,axis=0) #(100, self.neg_corr, window_size)\n",
    "        return {'target':target_np,\n",
    "                'pos':pos_np,\n",
    "                'neg':neg_np}\n",
    "    \n",
    "    def get_dataset(self, window_size, forecast_range = 20, corr_length_ratio = 1.5):\n",
    "        W = window_size\n",
    "        corr_length = round(W * corr_length_ratio)\n",
    "        dataset_length = self.dataset_asset.shape[0]\n",
    "        \n",
    "        targets = []\n",
    "        positives = []\n",
    "        negatives = []\n",
    "        indexes = []\n",
    "        Ys = []\n",
    "        for start_idx in tqdm(range(dataset_length-corr_length-forecast_range),desc='create dataset... '):\n",
    "            calc_data = self.calculate_correlation(self.dataset_asset_scaled.iloc[start_idx:start_idx+corr_length,:], W)\n",
    "            index_ = self.dataset_index_scaled.iloc[start_idx+corr_length-W:start_idx+corr_length,:].copy().to_numpy().T#(인덱스 개수, window_size)\n",
    "            index_data=[index_[np.newaxis,:,:].copy() for _ in range(100)]\n",
    "            index_data=np.concatenate(index_data,axis=0)#(100,인덱스 개수, window_size)\n",
    "            #총 길이 forecast_range+1이 되도록(일단은)\n",
    "            Y = self.dataset_asset_scaled.iloc[start_idx+corr_length-1:start_idx+corr_length+forecast_range,:].copy().to_numpy().T#(100,forecast_range+1)\n",
    "            \n",
    "            targets.append(calc_data['target'].copy())\n",
    "            positives.append(calc_data['pos'].copy())\n",
    "            negatives.append(calc_data['neg'].copy())\n",
    "            indexes.append(index_data.copy())\n",
    "            Ys.append(Y.copy())\n",
    "            \n",
    "        targets=np.concatenate(targets,axis=0)\n",
    "        positives=np.concatenate(positives,axis=0)\n",
    "        negatives=np.concatenate(negatives,axis=0)\n",
    "        indexes=np.concatenate(indexes,axis=0)\n",
    "        Ys=np.concatenate(Ys,axis=0)\n",
    "        \n",
    "        return {'target':targets,\n",
    "                'positive':positives,\n",
    "                'negative':negatives,\n",
    "                'index':indexes,\n",
    "                'Y':Ys}\n",
    "\n",
    "    \n",
    "    def load_file(self, file_list, sub_path):\n",
    "        ASSET = file_list\n",
    "\n",
    "        dataset_pd = None\n",
    "        for asset in ASSET:\n",
    "            data_one_asset = pd.read_csv(f\"{self.__file_path}{sub_path}/{asset}.csv\")[['Date','Adj Close']]\n",
    "            data_one_asset.columns = ['Date',asset]\n",
    "            if dataset_pd is None:\n",
    "                dataset_pd = data_one_asset.copy()\n",
    "            else:\n",
    "                dataset_pd = pd.merge(left = dataset_pd, right = data_one_asset, how = 'outer', on = 'Date')\n",
    "        dataset_pd['Date'] = pd.to_datetime(dataset_pd['Date'])\n",
    "        dataset_pd.sort_values(by=['Date'],inplace=True)\n",
    "        dataset_pd.reset_index(drop=True,inplace = True)\n",
    "        dataset_pd.fillna(method='ffill',inplace=True)\n",
    "        dataset_pd.fillna(method='bfill',inplace=True)\n",
    "        return dataset_pd\n",
    "    \n",
    "    def apply_scale(self):\n",
    "        dataset, indexset = self.dataset_asset.copy(), self.dataset_index.copy()\n",
    "        \n",
    "        merge_total = pd.merge(left=dataset, right=indexset, how='left', on='Date')\n",
    "        merge_total.sort_values(by=['Date'], inplace=True)\n",
    "        merge_total.fillna(method='ffill', inplace=True)\n",
    "        merge_total.fillna(method='bfill', inplace=True)\n",
    "        \n",
    "        dataset_scaler = MinMaxScaler(feature_range=(-1, +1))\n",
    "        indexset_scaler = MinMaxScaler(feature_range=(-1, +1))\n",
    "        \n",
    "        dataset_scaler.fit(merge_total[dataset.columns[1:]])\n",
    "        indexset_scaler.fit(merge_total[indexset.columns[1:]])\n",
    "        \n",
    "        merge_total[dataset.columns[1:]] = dataset_scaler.transform(merge_total[dataset.columns[1:]])\n",
    "        merge_total[indexset.columns[1:]] = indexset_scaler.transform(merge_total[indexset.columns[1:]])\n",
    "        \n",
    "        scaled_dataset = merge_total[dataset.columns[1:]].copy()\n",
    "        scaled_indexset = merge_total[indexset.columns[1:]].copy()\n",
    "        \n",
    "        self.dataset_asset_scaled = scaled_dataset\n",
    "        self.dataset_index_scaled = scaled_indexset\n",
    "        self.asset_scaler = dataset_scaler\n",
    "        self.index_scaler = indexset_scaler\n",
    "        self.Date_range = merge_total['Date']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d91dc5-d61c-47a2-a5f6-dbfd2aa5fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset = dataset_for_transformer(asset_list=M6_Universe.symbol, \n",
    "                                        index_list=Index_list, \n",
    "                                        pos_corr=10, \n",
    "                                        neg_corr=10)\n",
    "model_dataset.apply_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722e4bd-7106-47c1-984b-4e64376e3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = model_dataset.get_dataset(window_size=25, \n",
    "                                forecast_range = 20, \n",
    "                                corr_length_ratio = 1.5)\n",
    "print(final_dataset['target'].shape,final_dataset['positive'].shape,final_dataset['negative'].shape,final_dataset['index'].shape,final_dataset['Y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fe607-be8e-4998-bc0a-6c59b26ea827",
   "metadata": {},
   "source": [
    "## 데이터 셋 폼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f197d8b-b02c-4a99-a89d-69c53ad56e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n",
    "# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n",
    "# inputs,pos_inputs,neg_inputs,index_inputs, dec_inputs\n",
    "\n",
    "split_point=round(final_dataset['Y'].shape[0]*0.8)\n",
    "print(split_point)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': final_dataset['target'][:split_point],\n",
    "        'pos_inputs':final_dataset['positive'][:split_point],\n",
    "        'neg_inputs':final_dataset['negative'][:split_point],\n",
    "        'index_inputs':final_dataset['index'][:split_point],\n",
    "        'dec_inputs': final_dataset['Y'][:split_point, :-1] # 디코더의 입력. 마지막 값이 제거된다.(t 시점 ~ t+19 시점)\n",
    "    },\n",
    "    {\n",
    "        'outputs': final_dataset['Y'][:split_point, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 input의 마지막 값이 제거된다.(t+1 시점 ~ t+20 시점)\n",
    "    },\n",
    "))\n",
    "\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97841f72-9e5d-481c-8a22-a2ca37c31a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': final_dataset['target'][split_point:],\n",
    "        'pos_inputs':final_dataset['positive'][split_point:],\n",
    "        'neg_inputs':final_dataset['negative'][split_point:],\n",
    "        'index_inputs':final_dataset['index'][split_point:],\n",
    "        'dec_inputs': final_dataset['Y'][split_point:, :-1] # 디코더의 입력. 마지막 값이 제거된다.(t 시점 ~ t+19 시점)\n",
    "    },\n",
    "    {\n",
    "        'outputs': final_dataset['Y'][split_point:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 input의 마지막 값이 제거된다.(t+1 시점 ~ t+20 시점)\n",
    "    },\n",
    "))\n",
    "\n",
    "test_dataset = test_dataset.cache()\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd12c21-b002-44a1-aa61-2567bd1b69e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa31ff2f-3564-4dc0-88ab-a0fd3f1f7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "WINDOW_SIZE = 25\n",
    "FORECAST_RANGE = 20\n",
    "NUM_LAYERS = 4  #인코더, 디코더의 층 수\n",
    "DFF = 128 #트랜스포머 내부의 피드-포워드 신경망의 은닉층 크기(입력층, 출력층은 d_model) \n",
    "D_MODEL = 64 #인코더, 디코더에서의 정해진 출력 크기 = 임베딩 벡터의 차원\n",
    "NUM_HEADS = 8 #트랜스포머 내 어텐션에서 여러개로 분할 후 병렬로 어탠션 수행함. 이때의 병렬의 수\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = multi_enc_transformer(\n",
    "    window_size = WINDOW_SIZE,\n",
    "    forecast_range = FORECAST_RANGE,\n",
    "    P=10,\n",
    "    N=10,\n",
    "    I=7,\n",
    "    num_layers = NUM_LAYERS,\n",
    "    dff = DFF,\n",
    "    d_model = D_MODEL,\n",
    "    num_heads = NUM_HEADS,\n",
    "    dropout = DROPOUT,\n",
    "    name=\"stock_transformer\")\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='multi_encoder_transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d76f0-f4e3-4c12-926a-6ccd761127ed",
   "metadata": {},
   "source": [
    "# 모델 학습!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0912e-3a99-46dc-a1fa-87c4562cbacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "#              loss = tf.keras.losses.MeanSquaredError(),\n",
    "#              metrics=[tf.keras.metrics.RootMeanSquaredError(),\n",
    "#                       tf.keras.metrics.MeanAbsolutePercentageError()]\n",
    "#             )\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.MeanSquaredError()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73440767-9708-4597-89d1-232892b533b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_path = './transformer/ver_0820'\n",
    "\n",
    "os.makedirs(f'{callback_path}/ckpt', exist_ok=True)\n",
    "ckpt_path = callback_path+'/ckpt/ckpt_{epoch:02d}-{val_loss:.4f}'\n",
    "callback_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    ckpt_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only = True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "os.makedirs(f'{callback_path}/tsbd', exist_ok=True)\n",
    "callback_tsbd = tf.keras.callbacks.TensorBoard(f'{callback_path}/tsbd')\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.75 ** (epoch-5)\n",
    "callback_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710c458-9ebb-4339-b3e3-d290cb42a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "e = np.arange(15)\n",
    "\n",
    "r = [scheduler(e_,lr) for e_ in e]\n",
    "plt.plot(e,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c5e9e-8ad9-4a2d-a9d6-d1095570c736",
   "metadata": {},
   "source": [
    "한 에폭에 약 400s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b345d7-dcda-4fce-bb16-cfc5d6a15b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=15, validation_data=test_dataset, verbose = 1, callbacks=[callback_ckpt,callback_scheduler,callback_tsbd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96ee5a-028c-46ed-a969-1dba3db90b89",
   "metadata": {},
   "source": [
    "```bash\n",
    "tensorboard --logdir=~/바탕화면/main_drive/LeeJaeYong/2.Transformer/transformer/ver1/tsbd\n",
    "```\n",
    "-------\n",
    "혹시 이런 에러가 뜬다면...\n",
    "> E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n",
    "\n",
    "```bash\n",
    "/usr/bin/google-chrome-stable --enable-blink-features=ShadowDOMV0,CustomElementsV0,HTMLImports\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62e3c6-1a8c-4a35-a416-7f8ac4e17fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M6_LJY",
   "language": "python",
   "name": "m6_ljy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
