import tensorflow as tf


import numpy as np
import pandas as pd
import os

from util import *


class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, embedding_dim, num_heads=8):
        super(MultiHeadAttention, self).__init__()
        self.embedding_dim = embedding_dim # d_model
        self.num_heads = num_heads

        assert embedding_dim % self.num_heads == 0

        self.projection_dim = embedding_dim // num_heads
        self.query_dense = tf.keras.layers.Dense(embedding_dim)
        self.key_dense = tf.keras.layers.Dense(embedding_dim)
        self.value_dense = tf.keras.layers.Dense(embedding_dim)
        self.dense = tf.keras.layers.Dense(embedding_dim)

    def scaled_dot_product_attention(self, query, key, value):
        matmul_qk = tf.matmul(query, key, transpose_b=True)
        depth = tf.cast(tf.shape(key)[-1], tf.float32)
        logits = matmul_qk / tf.math.sqrt(depth)
        attention_weights = tf.nn.softmax(logits, axis=-1)
        output = tf.matmul(attention_weights, value)
        return output, attention_weights

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        # x.shape = [batch_size, seq_len, embedding_dim]
        batch_size = tf.shape(inputs)[0]

        # (batch_size, seq_len, embedding_dim)
        query = self.query_dense(inputs)
        key = self.key_dense(inputs)
        value = self.value_dense(inputs)

        # (batch_size, num_heads, seq_len, projection_dim)
        query = self.split_heads(query, batch_size)  
        key = self.split_heads(key, batch_size)
        value = self.split_heads(value, batch_size)

        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)
        # (batch_size, seq_len, num_heads, projection_dim)
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  

        # (batch_size, seq_len, embedding_dim)
        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))
        outputs = self.dense(concat_attention)
        return outputs


class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embedding_dim, num_heads, dff, rate=0.1,**kwargs):
        super(TransformerBlock, self).__init__(**kwargs)
        self.att = MultiHeadAttention(embedding_dim, num_heads)
        self.ffn = tf.keras.Sequential(
            [tf.keras.layers.Dense(dff, activation="relu"),
             tf.keras.layers.Dense(embedding_dim),]
        )
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs) # 첫번째 서브층 : 멀티 헤드 어텐션
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output) # Add & Norm
        ffn_output = self.ffn(out1) # 두번째 서브층 : 포지션 와이즈 피드 포워드 신경망
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output) # Add & Norm


class TokenAndPositionEmbedding(tf.keras.layers.Layer):
    def __init__(self, max_len, vocab_size, embedding_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim)

    def call(self, x):
        max_len = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=max_len, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions


vocab_size = 20000  # 빈도수 상위 2만개의 단어만 사용
max_len = 200  # 문장의 최대 길이

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)
print('훈련용 리뷰 개수 : {}'.format(len(X_train)))
print('테스트용 리뷰 개수 : {}'.format(len(X_test)))


X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)
X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)


X_train[0]


path = "./stock"
csvList=os.listdir(path+"/asset")

#include_list = []

for csv in csvList:
    if csv[-4:] != ".csv":
        continue

    data=pd.read_csv(path+"/asset"+'/'+csv)
    if(len(data)>2000):
        data=data[-2000-1:-1]
        data=data.reset_index()
        data=data[['Date','Adj Close']]
        data.columns = ['Date',csv[:-4]]

        #include_list.append(csv[:-4])

        if csv == csvList[0]:
            dataframe = data
        else:
            dataframe = pd.merge(dataframe,data,how="outer",on="Date")

dataframe["Date"] = pd.to_datetime(dataframe["Date"])
dataframe.sort_values(by=['Date'],inplace=True)
dataframe.fillna(method='ffill',inplace=True)
dataframe.fillna(method='bfill',inplace=True)
sort_asset = dataframe.columns.to_list()[1:]
sort_asset.sort()
sort_col = ["Date"]
sort_col.extend(sort_asset)
dataframe = dataframe[sort_col]

print(f'period : {dataframe["Date"].iloc[0]} ~ {dataframe["Date"].iloc[-1]}')
dataT=dataframe.drop(columns = "Date").copy()


dataframe


start_date='2012-03-01'
end_date="2022-05-01"
INDEX_DATA = make_index_data(start_date, end_date)


ABBV = pd.read_csv("./stock/asset/ABBV.csv")
ABBV


class one_asset:
    def __init__(self, asset_dataframe, X_col=["Adj Close"], window_size=20 ,H = 1, train_test_ratio = 0.8):
        self.origin_data = asset_dataframe
        self.X_col = X_col
        self.window_size = window_size
        self.H = H
        self.train_test_ratio = train_test_ratio
        self.__split_point = int(self.origin_data.shape[0]*0.7)

    def get_data(self, is_numpy = True, is_train = True):
        if is_train:
            dataframe = self.origin_data.iloc[:self.__split_point,:].copy()
        else:
            dataframe = self.origin_data.iloc[self.__split_point-self.window_size-self.H-1:,:].copy()
        dataframe.fillna(method='ffill',inplace=True)
        dataframe.fillna(method='bfill',inplace=True)
        X_data = dataframe[self.X_col].iloc[:-self.H,:].copy()
        Y_data = dataframe[["Adj Close"]].iloc[self.H + self.window_size-1:,:].copy()

        if is_numpy:
            return X_data.to_numpy(), Y_data.to_numpy()
        else:
            return X_data, Y_data

    def get_data_for_model(self, is_train = True, is_shuffle=True):
        X_np,Y_np = self.get_data(is_train = is_train)
        if is_train:
            start_point_array = np.arange(Y_np.shape[0])
            if is_shuffle:
                np.random.shuffle(start_point_array)
            X_data = []
            for sp in start_point_array:
                X_data.append(list(X_np[sp:sp+20]))
            X_data = np.array(X_data)
            Y_data = Y_np[start_point_array]

            return X_data, Y_data
        else:
            start_point_array = np.arange(Y_np.shape[0])
            X_data = []
            for sp in start_point_array:
                X_data.append(X_np[sp:sp+20])
            X_data = np.array(X_data)
            Y_data = Y_np

            return X_data, Y_data


abbv_1 = one_asset(ABBV,X_col=["Adj Close"], window_size=20 ,H = 1, train_test_ratio = 0.8)
X,Y = abbv_1.get_data(is_train = True)
print(X.shape, Y.shape)
X_1,Y_1 = abbv_1.get_data_for_model(is_train = True)
print(X_1.shape,Y_1.shape)


print(X.squeeze()[:24], Y.squeeze()[:4])


#max_len = 문장 최대 길이 -> window_size로
#vocab_size = 빈도수 상위 2만개의 단어만 사용 -> 뭐로 해야하지? window_size 통일?
#어쩌면 필요없을지도?? : 토큰을 안쓰니까?  -> Dense레이어와 같은 기능이니 Dense를 쓰자!!!

class AssetAndPositionEmbedding(tf.keras.layers.Layer):
    def __init__(self, window_size, embedding_dim,**kwargs):
        super(AssetAndPositionEmbedding, self).__init__(**kwargs)
        self.esset_emb = tf.keras.layers.Dense(embedding_dim) #입력 데이터 shape : (batch_size, window_size , 1)
        self.pos_emb = tf.keras.layers.Embedding(window_size, embedding_dim)
        self.__windowsize = window_size

    def call(self, x):
        positions = tf.range(start=0, limit=self.__windowsize, delta=1)
        positions = self.pos_emb(positions)
        x = tf.reshape(x,[-1,self.__windowsize,1])
        x = self.esset_emb(x)
        return x + positions


X_train[0].shape


tmp = ABBV["Adj Close"][:15].to_numpy().reshape((3,5,-1))

l = tf.keras.layers.Dense(10)
l2 = tf.keras.layers.Embedding(5, 10)
print(tmp)

positions = tf.range(start=0, limit=5, delta=1)
positions = l2(positions)
tmp = l(tmp)
print(tmp + positions)


embedding_dim = 32  # 각 단어의 임베딩 벡터의 차원
num_heads = 2  # 어텐션 헤드의 수
dff = 32  # 포지션 와이즈 피드 포워드 신경망의 은닉층의 크기

window_size = 25

#inputs = tf.keras.layers.Input(shape=(max_len,))
inputs = tf.keras.layers.Input(shape=(window_size,),name="input")
#embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)
embedding_layer = AssetAndPositionEmbedding(window_size, embedding_dim,name = "asset_and_position_Embedding")
x = embedding_layer(inputs)
transformer_block = TransformerBlock(embedding_dim, num_heads, dff, name = "transformer_block")
x = transformer_block(x)
x = tf.keras.layers.GlobalAveragePooling1D(name = "pooling_1")(x)
x = tf.keras.layers.Dropout(0.1,name="drop_out_1")(x)
x = tf.keras.layers.Dense(20, activation="relu",name="Dense_1")(x)
x = tf.keras.layers.Dropout(0.1,name="drop_out_2")(x)
outputs = tf.keras.layers.Dense(2, activation="softmax",name="Dense_2")(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)


model.compile(optimizer = tf.keras.optimizers.Adam(),
              loss = tf.keras.losses.MeanSquaredError(),
              metrics=[tf.keras.metrics.RootMeanSquaredError(),
                       tf.keras.metrics.MeanAbsolutePercentageError()]
             )
tf.keras.utils.plot_model(model, show_shapes=True)


history = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test))

print("테스트 정확도: %.4f" % (model.evaluate(X_test, y_test)[1]))


history.history
