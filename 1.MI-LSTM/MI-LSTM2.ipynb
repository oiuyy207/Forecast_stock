{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41eed282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 22:44:29.653967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#import data as d\n",
    "#import model\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0cdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "\n",
    "rnn=tf.compat.v1.nn.rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd65c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "path = './stock'\n",
    "path_asset = './stock/asset'\n",
    "\n",
    "INDEX_DATA = make_index_data(start_date='2012-03-01')\n",
    "\n",
    "timesize=[20+5*i for i in range(10)]\n",
    "timesize_for_calc_correlation=70\n",
    "positive_correlation_stock_num=30\n",
    "negative_correlation_stock_num=30\n",
    "train_test_rate=0.8\n",
    "batch_size=3072\n",
    "\n",
    "adam_lr = 0.001\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de5e67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (path is None) or (path_asset is None):\n",
    "    path, path_asset, _ = make_dataset_for_M6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d22673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class base_LSTMCell(rnn.BasicLSTMCell):\n",
    "    def __call__(self,inputs,state,scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = tf.split(1, 2, state)\n",
    "            concat = tf.layers.dense(tf.concat([inputs, h],axis=1), 4 * self._num_units)\n",
    "\n",
    "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i, j, f, o = tf.split(concat, 4, 1)\n",
    "\n",
    "            new_c = (c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) *\n",
    "                    self._activation(j))\n",
    "            new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = rnn.LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = tf.concat(1, [new_c, new_h])\n",
    "        return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdece64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MI_LSTMCell(rnn.BasicLSTMCell):\n",
    "    \"\"\"\n",
    "    Multi-Input LSTM proposed in the paper, Stock Price Prediction Using Attention-based Multi-Input LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               num_units,\n",
    "               num_inputs,\n",
    "               forget_bias=1.0,\n",
    "               state_is_tuple=True,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               name=None,\n",
    "               dtype=None,\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the basic LSTM cell.\n",
    "        args:\n",
    "            num_inputs: MI-LSTM의 입력의 개수. \n",
    "                이 파라미터에 따라 입력 게이트의 어텐션 레이어를 설정.\n",
    "                최소 1개이상.\n",
    "                1개일 경우, 어텐션 레이어를 제외하고 기본 LSTM과 동일.\n",
    "        \"\"\"        \n",
    "        super(MI_LSTMCell,self).__init__(num_units,\n",
    "               forget_bias=1.0,\n",
    "               state_is_tuple=True,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               name=None,\n",
    "               dtype=None,\n",
    "               **kwargs)\n",
    "        \n",
    "        if(type(num_inputs) is not int):\n",
    "            raise ValueError(\"num_inputs should be integer\")\n",
    "        if(num_inputs < 1):\n",
    "            raise ValueError(\"num_inputs should not be less than 0\")\n",
    "        self.num_inputs = num_inputs\n",
    "        self.alpha_weight=self.add_variable('alpha_weight',shape=[self._num_units,self._num_units])\n",
    "        self.alpha_bias=[]\n",
    "        for i in range(self.num_inputs):\n",
    "            self.alpha_bias.append(self.add_variable('alpha_bias'+str(i),shape=[1],initializer=tf.zeros_initializer()))\n",
    "\n",
    "    def __call__(self,inputs,state,scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.compat.v1.variable_scope(scope or type(self).__name__) as scope:  # \"BasicLSTMCell\"\n",
    "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = tf.split(1, 2, state)\n",
    "            inputs_list = tf.split(inputs,self.num_inputs,1)\n",
    "            #scope.reuse_variables()\n",
    "            concat = tf.compat.v1.layers.dense(tf.concat([inputs_list[0], h],axis=1), (3+self.num_inputs) * self._num_units)\n",
    "                                 \n",
    "            # 0 = forget_gate, 1 = output_gate, 2= main_new_input, 3 = main_input_gate, 4~ = input_gate_for_auxiliary\n",
    "            main_list = tf.split(concat, 3+self.num_inputs, 1)\n",
    "                        \n",
    "            #new_input_gate= list of all new_input.\n",
    "            new_input_gate=[tf.tanh(main_list[2])]\n",
    "            #linear layer for auxiliary inputs.\n",
    "            for i in range(1,self.num_inputs):\n",
    "                new_input_gate.append(tf.compat.v1.layers.dense(tf.concat([inputs_list[i], h],axis=1),self._num_units,activation=tf.tanh))\n",
    "\n",
    "            #making list of l. l = sigmoid(input_gate) * tanh(new_input)\n",
    "            new_l=[]\n",
    "            for i,new_input in enumerate(new_input_gate,3):\n",
    "                new_l.append(tf.sigmoid(main_list[i]) * new_input)\n",
    "\n",
    "\n",
    "            #making list of u.            \n",
    "            u=[]\n",
    "            for i,l in enumerate(new_l):\n",
    "                #temp = transpos(l) X W X Cell_State.\n",
    "                temp1=tf.matmul(l,self.alpha_weight)\n",
    "                temp1=tf.expand_dims(temp1,1)\n",
    "                temp2=tf.matmul(temp1,tf.expand_dims(c,2))\n",
    "                u.append(tf.tanh(tf.squeeze(temp2+self.alpha_bias[i],axis=2)))\n",
    "\n",
    "            #making list of alpha.\n",
    "            alpha=tf.nn.softmax(u,axis=0)\n",
    "\n",
    "            #making L.\n",
    "            L=[]\n",
    "            for i,l in enumerate(new_l):\n",
    "                L.append(alpha[i]*l)\n",
    "            L=tf.reduce_sum(L,axis=0)\n",
    "\n",
    "\n",
    "            #new state = c(t-1) * f + L. new h = tanh(c) + sigmoid(o)\n",
    "            new_c = (c * tf.sigmoid(main_list[0] + self._forget_bias)+L)\n",
    "            new_h = self._activation(new_c) * tf.sigmoid(main_list[1])\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = rnn.LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = tf.concat(1, [new_c, new_h])\n",
    "        return new_h, new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d696a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Layer():\n",
    "    \"\"\"\n",
    "    어텐션 레이어.\n",
    "    (None, TimeWindow, hidden_unit_size) shape의 LSTM 출력을 입력으로 받아 (None, 1, hidden_unit_size)의 텐서 출력.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        timewindow_size,\n",
    "        input_hidden_unit_size,\n",
    "        attention_size=None):\n",
    "        \"\"\"\n",
    "        Setting parameter for attention layer.\n",
    "        args:\n",
    "            timewindow_size = time window size of previous lstm layer.\n",
    "            input_hidden_unit_size = hidden unit number of previous lstm layer.\n",
    "            attention_size = size of this attention. \n",
    "                default = input_hidden_unit_size.\n",
    "        \"\"\"\n",
    "        if(attention_size is None):\n",
    "            attention_size=input_hidden_unit_size\n",
    "        self.o_size=attention_size\n",
    "        self.h_size=input_hidden_unit_size\n",
    "        self.t_size=timewindow_size\n",
    "\n",
    "        self.beta_weight=tf.Variable(tf.random.normal([self.h_size,self.o_size]), name='beta_weight')\n",
    "        self.beta_bias=tf.Variable(tf.zeros([self.o_size]),name='beta_bias')\n",
    "\n",
    "        self.v=tf.Variable(tf.random.normal([self.o_size,1]),name='beta_v')\n",
    "\n",
    "    def __call__(self,inputs):\n",
    "        \"\"\"\n",
    "        producing output with actual inputs.\n",
    "        shape of output will be (batch_size, 1, input_hidden_unit_size).\n",
    "        \"\"\"\n",
    "        #temp = tanh(Y X W + b) ->shape of result = (-1, self.o_size)\n",
    "        temp=tf.matmul(tf.reshape(inputs,[-1,self.h_size]),self.beta_weight)\n",
    "         \n",
    "        temp=tf.tanh(temp+self.beta_bias)\n",
    "        \n",
    "            \n",
    "        #j=temp X v\n",
    "        j=tf.reshape(tf.matmul(temp,self.v),[-1,self.t_size,1])\n",
    "\n",
    "        beta=tf.nn.softmax(j)\n",
    "        \n",
    "        \n",
    "\n",
    "        output=beta*inputs\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff619efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(keras.Model):\n",
    "    \"\"\"\n",
    "    Basic LSTM list for test.\n",
    "    \"\"\"\n",
    "    def __init__(self,windowsize,Pos,Neg):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.T=windowsize\n",
    "        self.P=Pos\n",
    "        self.N=Neg\n",
    "\n",
    "        self.Y=tf.keras.layers.InputLayer(input_shape=(None,self.T,1),dtype = tf.float32)\n",
    "        self.Xp=tf.keras.layers.InputLayer(input_shape=(None,self.P,self.T,1),dtype = tf.float32)\n",
    "        self.Xn=tf.keras.layers.InputLayer(input_shape=(None,self.N,self.T,1),dtype = tf.float32)\n",
    "        self.Xi=tf.keras.layers.InputLayer(input_shape=(None,9,self.T,1),dtype = tf.float32)\n",
    "\n",
    "        self.LSTM1=tf.keras.layers.RNN(tf.keras.layers.LSTMCell(64,name='lstm1'),\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        \n",
    "        #MI-LSTM\n",
    "        self.LSTM2=tf.keras.layers.RNN(MI_LSTMCell(64,4,name='lstm2'),\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        \n",
    "        #Attention_Layer\n",
    "        self.attention_layer=Attention_Layer(self.T,64)\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.dense_layers = tf.keras.models.Sequential()\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    def call(self,y,xp,xn,xi):\n",
    "        Y_1,_,_=self.LSTM1(self.Y(y))\n",
    "        \n",
    "        Xis=tf.split(self.Xi(xi),9,1)\n",
    "        Xi_list=[]\n",
    "        for i in range(len(Xis)):\n",
    "            o,_,_=self.LSTM1(tf.squeeze(Xis[i],axis=1))\n",
    "            Xi_list.append(o)\n",
    "\n",
    "        Xps=tf.split(self.Xp(xp),self.P,1)\n",
    "        Xp_list=[]\n",
    "        for i in range(len(Xps)):\n",
    "            o,_,_=self.LSTM1(tf.squeeze(Xps[i],axis=1))\n",
    "            Xp_list.append(o)\n",
    "        \n",
    "        Xns=tf.split(self.Xn(xn),self.N,1)\n",
    "        Xn_list=[]\n",
    "        for i in range(len(Xns)):\n",
    "            o,_,_=self.LSTM1(tf.squeeze(Xns[i],axis=1))\n",
    "            Xn_list.append(o)\n",
    "        Xp_1=tf.reduce_mean(Xp_list,0)\n",
    "        Xn_1=tf.reduce_mean(Xn_list,0)\n",
    "        Xi_1=tf.reduce_mean(Xi_list,0)\n",
    "\n",
    "        result=tf.concat([Y_1,Xp_1,Xn_1,Xi_1],axis=2)\n",
    "       \n",
    "        Y_2,_,_ =self.LSTM2(result)\n",
    "        \n",
    "        Y_3=self.attention_layer(Y_2)\n",
    "\n",
    "        #Non-linear units for producing final prediction.\n",
    "        R_1 = self.flatten(Y_3)\n",
    "        R_6=self.dense_layers(R_1)\n",
    "\n",
    "        return R_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7016e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_model_iter(Window_size,Window_size_CC):\n",
    "    all_stock = StockData_3(path_asset,\n",
    "                            INDEX_DATA,\n",
    "                            Window_size,\n",
    "                            Window_size_CC,\n",
    "                            positive_correlation_stock_num,\n",
    "                            negative_correlation_stock_num,\n",
    "                            train_test_rate,\n",
    "                            batch_size,\n",
    "                            date_duration = 2000)\n",
    "    \n",
    "    scaler_info_csv(all_stock.scaler, all_stock.indexScaler, Window_size, all_stock.include_asset, all_stock.include_index)\n",
    "    \n",
    "    lstmModel=LSTM_Model(Window_size,\n",
    "                         positive_correlation_stock_num,\n",
    "                         negative_correlation_stock_num)\n",
    "        \n",
    "    result_dic={}\n",
    "        \n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    optimizer=tf.keras.optimizers.Adam(adam_lr)\n",
    "    evalution_costplt=[]\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        #epoch start\n",
    "        start_time = time.time()\n",
    "        training_cost=0\n",
    "        evalution_cost=0\n",
    "    \n",
    "        #training batch\n",
    "        opt = 'training'\n",
    "        for batch in tqdm(all_stock.getBatch('training'),unit=\" Batches\",desc=f\"{opt:>15}\"):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = lstmModel(batch['y'],batch['xp'],batch['xn'],batch['xi'])\n",
    "                loss_value = loss_fn(batch['target'], logits)\n",
    "                loss_value += sum(lstmModel.losses)\n",
    "            training_cost+=loss_value\n",
    "            grads = tape.gradient(loss_value, lstmModel.trainable_weights,unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "            optimizer.apply_gradients(zip(grads, lstmModel.trainable_weights))\n",
    "\n",
    "        #evaluation batch\n",
    "        opt = 'evaluation'\n",
    "        for batch in tqdm(all_stock.getBatch('evaluation'),unit=\" Batches\",desc=f\"{opt:>15}\"):\n",
    "            logits = lstmModel(batch['y'],batch['xp'],batch['xn'],batch['xi'])\n",
    "            loss_value = loss_fn(batch['target'], logits)\n",
    "            loss_value += sum(lstmModel.losses)\n",
    "            evalution_cost+=loss_value\n",
    "   \n",
    "        #epoch end\n",
    "        elapsed_time = time.time()-start_time\n",
    "        training_cost=training_cost/all_stock.batchNum['training']\n",
    "        evalution_cost=evalution_cost/all_stock.batchNum['evaluation']\n",
    "        result_dic[i]=[training_cost,evalution_cost]\n",
    "        evalution_costplt.append(evalution_cost)\n",
    "\n",
    "        print(f'epoch : {i}, t_cost : {training_cost:0.6f}, e_cost : {evalution_cost:0.6f}, elapsed time : {elapsed_time:0.2f}sec')\n",
    "\n",
    "    #Finish epoch\n",
    "    ## save model\n",
    "    os.makedirs(path+\"/model_save\",exist_ok=True)\n",
    "    lstmModel.save(f\"./model_save/windowsize_{Window_size}\")\n",
    "    \n",
    "    sorted_result=sorted(result_dic,key=lambda k:result_dic[k][1])\n",
    "    bestEpoch=sorted_result[0]\n",
    "    print(f'\\n#Best result at epoch {bestEpoch}')\n",
    "    print(f't_cost : {result_dic[bestEpoch][0]:0.6f}, e_cost : {result_dic[bestEpoch][1]:0.6f}')\n",
    "\n",
    "    plt.plot(evalution_costplt)\n",
    "    \n",
    "    os.makedirs(path+\"/evalution_costplt\",exist_ok=True)\n",
    "    plt.savefig(f'./evalution_costplt/windowsize_{Window_size}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69955a",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae168ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in timesize:\n",
    "    t = \" WindowSize \"+str(w)+\" \"\n",
    "    print(f\"{t:=^40}\")\n",
    "    one_model_iter(w,timesize_for_calc_correlation)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db89e82-61bf-453f-a3ee-d16b7090300c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockPrice shape:  (2000, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "making dataset progress: 100%|█| 1930/1930 [03:35<\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(        scale_      min_\n",
       " ABBV  0.014573 -1.528193\n",
       " ACN   0.005767 -1.382248\n",
       " AEP   0.030307 -2.141966\n",
       " AIZ   0.014183 -1.728744\n",
       " ALLE  0.019082 -1.801322\n",
       " ...        ...       ...\n",
       " XLY   0.013051 -1.750794\n",
       " XLI   0.031247 -2.333929\n",
       " XLU   0.043548 -2.351434\n",
       " XLP   0.044041 -2.548361\n",
       " XLB   0.034746 -2.146241\n",
       " \n",
       " [84 rows x 2 columns],\n",
       "             scale_      min_\n",
       " SP500     0.000569 -1.724737\n",
       " Dow       0.000081 -1.980254\n",
       " Nasdaq    0.000142 -1.347734\n",
       " CrudeOil  0.012397 -0.533503\n",
       " Gold      0.001999 -3.100130\n",
       " Silver    0.083766 -1.982995\n",
       " EURUSD    5.631751 -6.845521\n",
       " gsci      0.003398 -1.835542\n",
       " vix       0.027192 -1.248538)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Window_size = 20\n",
    "Window_size_CC = 70\n",
    "all_stock = StockData_3(path_asset,\n",
    "                            INDEX_DATA,\n",
    "                            Window_size,\n",
    "                            Window_size_CC,\n",
    "                            positive_correlation_stock_num,\n",
    "                            negative_correlation_stock_num,\n",
    "                            train_test_rate,\n",
    "                            batch_size,\n",
    "                            date_duration = 2000)\n",
    "    \n",
    "scaler_info_csv(all_stock.scaler, all_stock.indexScaler, Window_size, all_stock.include_asset, all_stock.include_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d4a26-b938-4ee7-8ded-cb2ebaf5a6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M6_LJY",
   "language": "python",
   "name": "m6_ljy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
