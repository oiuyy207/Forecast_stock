{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1653449407275,
     "user": {
      "displayName": "HCturtle",
      "userId": "11504172790621175694"
     },
     "user_tz": -540
    },
    "id": "PGIdqjoDa5uO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#import data as d\n",
    "#import model\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 504,
     "status": "ok",
     "timestamp": 1653449410902,
     "user": {
      "displayName": "HCturtle",
      "userId": "11504172790621175694"
     },
     "user_tz": -540
    },
    "id": "OJlLutoQbKlZ"
   },
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 07:02:14.649774: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-26 07:02:14.650082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-26 07:02:14.650693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:06.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-05-26 07:02:14.650824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-26 07:02:14.652065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:00:07.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-05-26 07:02:14.652107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-26 07:02:14.652152: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-26 07:02:14.652167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-05-26 07:02:14.652181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-26 07:02:14.652195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-26 07:02:14.652209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-26 07:02:14.652223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-26 07:02:14.652237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-26 07:02:14.652311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-26 07:02:14.652846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-26 07:02:14.654112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-26 07:02:14.654654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-26 07:02:14.655864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#tf.compat.v1.disable_eager_execution()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdevice_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_local_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/M6_LJY/lib/python3.8/site-packages/tensorflow/python/client/device_lib.py:43\u001b[0m, in \u001b[0;36mlist_local_devices\u001b[0;34m(session_config)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m   serialized_config \u001b[38;5;241m=\u001b[39m session_config\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m---> 43\u001b[0m     _convert(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_pywrap_device_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserialized_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m ]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "#22-05-26 현재 피드 : 44646\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61088,
     "status": "ok",
     "timestamp": 1653449471988,
     "user": {
      "displayName": "HCturtle",
      "userId": "11504172790621175694"
     },
     "user_tz": -540
    },
    "id": "1W850S7jdcaN",
    "outputId": "74e1d0b7-34ca-4488-b08d-eeaf98b0ef1f"
   },
   "outputs": [],
   "source": [
    "path, path_asset, _ = make_dataset_for_M6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1730,
     "status": "ok",
     "timestamp": 1653449790975,
     "user": {
      "displayName": "HCturtle",
      "userId": "11504172790621175694"
     },
     "user_tz": -540
    },
    "id": "jCy6JVy1_vlt",
    "outputId": "958d9152-7ea8-454b-ae11-055f5037f25d"
   },
   "outputs": [],
   "source": [
    "INDEX_DATA = make_index_data(start_date='2012-03-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './stock'\n",
    "path_asset = './stock/asset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn=tf.compat.v1.nn.rnn_cell\n",
    "class base_LSTMCell(rnn.BasicLSTMCell):\n",
    "    def __call__(self,inputs,state,scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = tf.split(1, 2, state)\n",
    "            concat = tf.layers.dense(tf.concat([inputs, h],axis=1), 4 * self._num_units)\n",
    "\n",
    "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i, j, f, o = tf.split(concat, 4, 1)\n",
    "\n",
    "            new_c = (c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) *\n",
    "                    self._activation(j))\n",
    "            new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = rnn.LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = tf.concat(1, [new_c, new_h])\n",
    "        return new_h, new_state\n",
    "\n",
    "class MI_LSTMCell(rnn.BasicLSTMCell):\n",
    "    \"\"\"\n",
    "    Multi-Input LSTM proposed in the paper, Stock Price Prediction Using Attention-based Multi-Input LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               num_units,\n",
    "               num_inputs,\n",
    "               forget_bias=1.0,\n",
    "               state_is_tuple=True,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               name=None,\n",
    "               dtype=None,\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the basic LSTM cell.\n",
    "        args:\n",
    "            num_inputs: MI-LSTM의 입력의 개수. \n",
    "                이 파라미터에 따라 입력 게이트의 어텐션 레이어를 설정.\n",
    "                최소 1개이상.\n",
    "                1개일 경우, 어텐션 레이어를 제외하고 기본 LSTM과 동일.\n",
    "        \"\"\"        \n",
    "        super(MI_LSTMCell,self).__init__(num_units,\n",
    "               forget_bias=1.0,\n",
    "               state_is_tuple=True,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               name=None,\n",
    "               dtype=None,\n",
    "               **kwargs)\n",
    "        \n",
    "        if(type(num_inputs) is not int):\n",
    "            raise ValueError(\"num_inputs should be integer\")\n",
    "        if(num_inputs < 1):\n",
    "            raise ValueError(\"num_inputs should not be less than 0\")\n",
    "        self.num_inputs = num_inputs\n",
    "        self.alpha_weight=self.add_variable('alpha_weight',shape=[self._num_units,self._num_units])\n",
    "        self.alpha_bias=[]\n",
    "        for i in range(self.num_inputs):\n",
    "            self.alpha_bias.append(self.add_variable('alpha_bias'+str(i),shape=[1],initializer=tf.zeros_initializer()))\n",
    "\n",
    "    def __call__(self,inputs,state,scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.compat.v1.variable_scope(scope or type(self).__name__) as scope:  # \"BasicLSTMCell\"\n",
    "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = tf.split(1, 2, state)\n",
    "            inputs_list = tf.split(inputs,self.num_inputs,1)\n",
    "            #scope.reuse_variables()\n",
    "            concat = tf.compat.v1.layers.dense(tf.concat([inputs_list[0], h],axis=1), (3+self.num_inputs) * self._num_units)\n",
    "                                 \n",
    "            # 0 = forget_gate, 1 = output_gate, 2= main_new_input, 3 = main_input_gate, 4~ = input_gate_for_auxiliary\n",
    "            main_list = tf.split(concat, 3+self.num_inputs, 1)\n",
    "                        \n",
    "            #new_input_gate= list of all new_input.\n",
    "            new_input_gate=[tf.tanh(main_list[2])]\n",
    "            #linear layer for auxiliary inputs.\n",
    "            for i in range(1,self.num_inputs):\n",
    "                new_input_gate.append(tf.compat.v1.layers.dense(tf.concat([inputs_list[i], h],axis=1),self._num_units,activation=tf.tanh))\n",
    "\n",
    "            #making list of l. l = sigmoid(input_gate) * tanh(new_input)\n",
    "            new_l=[]\n",
    "            for i,new_input in enumerate(new_input_gate,3):\n",
    "                new_l.append(tf.sigmoid(main_list[i]) * new_input)\n",
    "\n",
    "\n",
    "            #making list of u.            \n",
    "            u=[]\n",
    "            for i,l in enumerate(new_l):\n",
    "                #temp = transpos(l) X W X Cell_State.\n",
    "                temp1=tf.matmul(l,self.alpha_weight)\n",
    "                temp1=tf.expand_dims(temp1,1)\n",
    "                temp2=tf.matmul(temp1,tf.expand_dims(c,2))\n",
    "                u.append(tf.tanh(tf.squeeze(temp2+self.alpha_bias[i],axis=2)))\n",
    "\n",
    "            #making list of alpha.\n",
    "            alpha=tf.nn.softmax(u,axis=0)\n",
    "\n",
    "            #making L.\n",
    "            L=[]\n",
    "            for i,l in enumerate(new_l):\n",
    "                L.append(alpha[i]*l)\n",
    "            L=tf.reduce_sum(L,axis=0)\n",
    "\n",
    "\n",
    "            #new state = c(t-1) * f + L. new h = tanh(c) + sigmoid(o)\n",
    "            new_c = (c * tf.sigmoid(main_list[0] + self._forget_bias)+L)\n",
    "            new_h = self._activation(new_c) * tf.sigmoid(main_list[1])\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = rnn.LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = tf.concat(1, [new_c, new_h])\n",
    "        return new_h, new_state\n",
    "\n",
    "\n",
    "class Attention_Layer():\n",
    "    \"\"\"\n",
    "    어텐션 레이어.\n",
    "    (None, TimeWindow, hidden_unit_size) shape의 LSTM 출력을 입력으로 받아 (None, 1, hidden_unit_size)의 텐서 출력.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        timewindow_size,\n",
    "        input_hidden_unit_size,\n",
    "        attention_size=None):\n",
    "        \"\"\"\n",
    "        Setting parameter for attention layer.\n",
    "        args:\n",
    "            timewindow_size = time window size of previous lstm layer.\n",
    "            input_hidden_unit_size = hidden unit number of previous lstm layer.\n",
    "            attention_size = size of this attention. \n",
    "                default = input_hidden_unit_size.\n",
    "        \"\"\"\n",
    "        if(attention_size is None):\n",
    "            attention_size=input_hidden_unit_size\n",
    "        self.o_size=attention_size\n",
    "        self.h_size=input_hidden_unit_size\n",
    "        self.t_size=timewindow_size\n",
    "\n",
    "        self.beta_weight=tf.Variable(tf.random.normal([self.h_size,self.o_size]), name='beta_weight')\n",
    "        self.beta_bias=tf.Variable(tf.zeros([self.o_size]),name='beta_bias')\n",
    "\n",
    "        self.v=tf.Variable(tf.random.normal([self.o_size,1]),name='beta_v')\n",
    "\n",
    "    def __call__(self,inputs):\n",
    "        \"\"\"\n",
    "        producing output with actual inputs.\n",
    "        shape of output will be (batch_size, 1, input_hidden_unit_size).\n",
    "        \"\"\"\n",
    "        #temp = tanh(Y X W + b) ->shape of result = (-1, self.o_size)\n",
    "        temp=tf.matmul(tf.reshape(inputs,[-1,self.h_size]),self.beta_weight)\n",
    "         \n",
    "        temp=tf.tanh(temp+self.beta_bias)\n",
    "        \n",
    "            \n",
    "        #j=temp X v\n",
    "        j=tf.reshape(tf.matmul(temp,self.v),[-1,self.t_size,1])\n",
    "\n",
    "        beta=tf.nn.softmax(j)\n",
    "        \n",
    "        \n",
    "\n",
    "        output=beta*inputs\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1653449733730,
     "user": {
      "displayName": "HCturtle",
      "userId": "11504172790621175694"
     },
     "user_tz": -540
    },
    "id": "wGGbRiehf_9_"
   },
   "outputs": [],
   "source": [
    "class LSTM_Model(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Basic LSTM list for test.\n",
    "    \"\"\"\n",
    "    def __init__(self,windowsize,Pos,Neg):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.T=windowsize\n",
    "        self.P=Pos\n",
    "        self.N=Neg\n",
    "\n",
    "        self.Y=tf.keras.layers.InputLayer(input_shape=(None,self.T,1),dtype = tf.float32)\n",
    "        self.Xp=tf.keras.layers.InputLayer(input_shape=(None,self.P,self.T,1),dtype = tf.float32)\n",
    "        self.Xn=tf.keras.layers.InputLayer(input_shape=(None,self.N,self.T,1),dtype = tf.float32)\n",
    "        self.Xi=tf.keras.layers.InputLayer(input_shape=(None,9,self.T,1),dtype = tf.float32)\n",
    "\n",
    "        self.LSTM1=tf.keras.layers.RNN(tf.keras.layers.LSTMCell(64,name='lstm1'),\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        \n",
    "        #MI-LSTM\n",
    "        self.LSTM2=tf.keras.layers.RNN(MI_LSTMCell(64,4,name='lstm2'),\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        \n",
    "        #Attention_Layer\n",
    "        self.attention_layer=Attention_Layer(self.T,64)\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.dense_layers = tf.keras.models.Sequential()\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    def call(self,y,xp,xn,xi):\n",
    "        Y_1,_,_=self.LSTM1(self.Y(y))\n",
    "        \n",
    "        Xis=tf.split(self.Xi(xi),9,1)\n",
    "        Xi_list=[]\n",
    "        for i in range(len(Xis)):\n",
    "            o,_,_=self.LSTM1(tf.squeeze(Xis[i],axis=1))\n",
    "            Xi_list.append(o)\n",
    "\n",
    "        Xps=tf.split(self.Xp(xp),self.P,1)\n",
    "        Xp_list=[]\n",
    "        for i in range(len(Xps)):\n",
    "            o,_,_=self.LSTM1(tf.squeeze(Xps[i],axis=1))\n",
    "            Xp_list.append(o)\n",
    "        \n",
    "        Xns=tf.split(self.Xn(xn),self.N,1)\n",
    "        Xn_list=[]\n",
    "        for i in range(len(Xns)):\n",
    "            o,_,_=self.LSTM1(tf.squeeze(Xns[i],axis=1))\n",
    "            Xn_list.append(o)\n",
    "        Xp_1=tf.reduce_mean(Xp_list,0)\n",
    "        Xn_1=tf.reduce_mean(Xn_list,0)\n",
    "        Xi_1=tf.reduce_mean(Xi_list,0)\n",
    "\n",
    "        result=tf.concat([Y_1,Xp_1,Xn_1,Xi_1],axis=2)\n",
    "       \n",
    "        Y_2,_,_ =self.LSTM2(result)\n",
    "        \n",
    "        Y_3=self.attention_layer(Y_2)\n",
    "\n",
    "        #Non-linear units for producing final prediction.\n",
    "        R_1 = self.flatten(Y_3)\n",
    "        R_6=self.dense_layers(R_1)\n",
    "\n",
    "        return R_6\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1653449733731,
     "user": {
      "displayName": "HCturtle",
      "userId": "11504172790621175694"
     },
     "user_tz": -540
    },
    "id": "4_G8JHmG9m9q"
   },
   "outputs": [],
   "source": [
    "#parameter list\n",
    "timesize=50 #20\n",
    "timesize_for_calc_correlation=100 #50\n",
    "positive_correlation_stock_num=30\n",
    "negative_correlation_stock_num=30\n",
    "train_test_rate=0.8\n",
    "batch_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243806,
     "status": "ok",
     "timestamp": 1653450063995,
     "user": {
      "displayName": "HCturtle",
      "userId": "11504172790621175694"
     },
     "user_tz": -540
    },
    "id": "gYyBiMjBQ0kC",
    "outputId": "682ce799-8df8-4367-e8eb-1c309e955844"
   },
   "outputs": [],
   "source": [
    "all_stock = StockData_2(path_asset,\n",
    "                        INDEX_DATA,\n",
    "                        timesize,\n",
    "                        timesize_for_calc_correlation,\n",
    "                        positive_correlation_stock_num,\n",
    "                        negative_correlation_stock_num,\n",
    "                        train_test_rate,\n",
    "                        batch_size,\n",
    "                        date_duration = 2000,\n",
    "                        h = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstmModel=LSTM_Model(\n",
    "        timesize,\n",
    "        positive_correlation_stock_num,\n",
    "        negative_correlation_stock_num\n",
    "        )\n",
    "\n",
    "result_dic={}\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer=tf.keras.optimizers.Adam(0.001)\n",
    "evalution_costplt=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8887,
     "status": "error",
     "timestamp": 1653450072877,
     "user": {
      "displayName": "HCturtle",
      "userId": "11504172790621175694"
     },
     "user_tz": -540
    },
    "id": "BllGaEvfr6mh",
    "outputId": "e2289f38-5ba9-405b-a446-be1011928ea2"
   },
   "outputs": [],
   "source": [
    "print('\\n#training#')\n",
    "\n",
    "for i in range(30):\n",
    "    #epoch start\n",
    "    start_time = time.time()\n",
    "    training_cost=0\n",
    "    evalution_cost=0\n",
    "    \n",
    "    #training batch\n",
    "    for batch in all_stock.getBatch('training'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = lstmModel(batch['y'],batch['xp'],batch['xn'],batch['xi'])\n",
    "            loss_value = loss_fn(batch['target'], logits)\n",
    "            loss_value += sum(lstmModel.losses)\n",
    "        training_cost+=loss_value\n",
    "        grads = tape.gradient(loss_value, lstmModel.trainable_weights,unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "        optimizer.apply_gradients(zip(grads, lstmModel.trainable_weights))\n",
    "\n",
    "    #evaluation batch\n",
    "    for batch in all_stock.getBatch('evaluation'):\n",
    "        logits = lstmModel(batch['y'],batch['xp'],batch['xn'],batch['xi'])\n",
    "        loss_value = loss_fn(batch['target'], logits)\n",
    "        loss_value += sum(lstmModel.losses)\n",
    "        evalution_cost+=loss_value\n",
    "\n",
    "    print(\"traning_cost : \" , training_cost)\n",
    "    print(\"evalution_cost : \" , evalution_cost)\n",
    "   \n",
    "    #epoch end\n",
    "    elapsed_time = time.time()-start_time\n",
    "    training_cost=training_cost/all_stock.batchNum['training']\n",
    "    evalution_cost=evalution_cost/all_stock.batchNum['evaluation']\n",
    "    result_dic[i]=[training_cost,evalution_cost]\n",
    "    evalution_costplt.append(evalution_cost)\n",
    "\n",
    "    print(f'epoch : {i}, t_cost : {training_cost:0.6f}, e_cost : {evalution_cost:0.6f}, elapsed time : {elapsed_time:0.2f}sec')\n",
    "#\n",
    "\n",
    "sorted_result=sorted(result_dic,key=lambda k:result_dic[k][1])\n",
    "bestEpoch=sorted_result[0]\n",
    "print(f'\\n#Best result at epoch {bestEpoch}')\n",
    "print(f't_cost : {result_dic[bestEpoch][0]:0.6f}, e_cost : {result_dic[bestEpoch][1]:0.6f}')\n",
    "\n",
    "plt.plot(evalution_costplt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3L_ioSrHe2o"
   },
   "outputs": [],
   "source": [
    "plt.savefig('30-mi_lstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lzZN_rrmyiw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM5cgsGf1PvE4cwErVmRzi7",
   "collapsed_sections": [],
   "mount_file_id": "1zFtG5K9iku-PAO_JVd53IU1N10z6ZQ4I",
   "name": "MI-LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "M6_LJY",
   "language": "python",
   "name": "m6_ljy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
