{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41eed282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 22:36:45.008097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#import data as d\n",
    "#import model\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0cdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "\n",
    "rnn=tf.compat.v1.nn.rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd65c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "path = './stock'\n",
    "path_asset = './stock/asset'\n",
    "\n",
    "INDEX_DATA = make_index_data(start_date='2012-03-01')\n",
    "\n",
    "timesize=[20+5*i for i in range(10)]\n",
    "timesize_for_calc_correlation=70\n",
    "positive_correlation_stock_num=30\n",
    "negative_correlation_stock_num=30\n",
    "train_test_rate=0.8\n",
    "batch_size=3072\n",
    "\n",
    "adam_lr = 0.001\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de5e67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (path is None) or (path_asset is None):\n",
    "    path, path_asset, _ = make_dataset_for_M6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d22673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class base_LSTMCell(rnn.BasicLSTMCell):\n",
    "    def __call__(self,inputs,state,scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = tf.split(1, 2, state)\n",
    "            concat = tf.layers.dense(tf.concat([inputs, h],axis=1), 4 * self._num_units)\n",
    "\n",
    "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i, j, f, o = tf.split(concat, 4, 1)\n",
    "\n",
    "            new_c = (c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) *\n",
    "                    self._activation(j))\n",
    "            new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = rnn.LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = tf.concat(1, [new_c, new_h])\n",
    "        return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdece64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MI_LSTMCell(rnn.BasicLSTMCell):\n",
    "    \"\"\"\n",
    "    Multi-Input LSTM proposed in the paper, Stock Price Prediction Using Attention-based Multi-Input LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               num_units,\n",
    "               num_inputs,\n",
    "               forget_bias=1.0,\n",
    "               state_is_tuple=True,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               name=None,\n",
    "               dtype=None,\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the basic LSTM cell.\n",
    "        args:\n",
    "            num_inputs: MI-LSTM의 입력의 개수. \n",
    "                이 파라미터에 따라 입력 게이트의 어텐션 레이어를 설정.\n",
    "                최소 1개이상.\n",
    "                1개일 경우, 어텐션 레이어를 제외하고 기본 LSTM과 동일.\n",
    "        \"\"\"        \n",
    "        super(MI_LSTMCell,self).__init__(num_units,\n",
    "               forget_bias=1.0,\n",
    "               state_is_tuple=True,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               name=None,\n",
    "               dtype=None,\n",
    "               **kwargs)\n",
    "        \n",
    "        if(type(num_inputs) is not int):\n",
    "            raise ValueError(\"num_inputs should be integer\")\n",
    "        if(num_inputs < 1):\n",
    "            raise ValueError(\"num_inputs should not be less than 0\")\n",
    "        self.num_inputs = num_inputs\n",
    "        self.alpha_weight=self.add_variable('alpha_weight',shape=[self._num_units,self._num_units])\n",
    "        self.alpha_bias=[]\n",
    "        for i in range(self.num_inputs):\n",
    "            self.alpha_bias.append(self.add_variable('alpha_bias'+str(i),shape=[1],initializer=tf.zeros_initializer()))\n",
    "\n",
    "    def __call__(self,inputs,state,scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.compat.v1.variable_scope(scope or type(self).__name__) as scope:  # \"BasicLSTMCell\"\n",
    "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = tf.split(1, 2, state)\n",
    "            inputs_list = tf.split(inputs,self.num_inputs,1)\n",
    "            #scope.reuse_variables()\n",
    "            concat = tf.compat.v1.layers.dense(tf.concat([inputs_list[0], h],axis=1), (3+self.num_inputs) * self._num_units)\n",
    "                                 \n",
    "            # 0 = forget_gate, 1 = output_gate, 2= main_new_input, 3 = main_input_gate, 4~ = input_gate_for_auxiliary\n",
    "            main_list = tf.split(concat, 3+self.num_inputs, 1)\n",
    "                        \n",
    "            #new_input_gate= list of all new_input.\n",
    "            new_input_gate=[tf.tanh(main_list[2])]\n",
    "            #linear layer for auxiliary inputs.\n",
    "            for i in range(1,self.num_inputs):\n",
    "                new_input_gate.append(tf.compat.v1.layers.dense(tf.concat([inputs_list[i], h],axis=1),self._num_units,activation=tf.tanh))\n",
    "\n",
    "            #making list of l. l = sigmoid(input_gate) * tanh(new_input)\n",
    "            new_l=[]\n",
    "            for i,new_input in enumerate(new_input_gate,3):\n",
    "                new_l.append(tf.sigmoid(main_list[i]) * new_input)\n",
    "\n",
    "\n",
    "            #making list of u.            \n",
    "            u=[]\n",
    "            for i,l in enumerate(new_l):\n",
    "                #temp = transpos(l) X W X Cell_State.\n",
    "                temp1=tf.matmul(l,self.alpha_weight)\n",
    "                temp1=tf.expand_dims(temp1,1)\n",
    "                temp2=tf.matmul(temp1,tf.expand_dims(c,2))\n",
    "                u.append(tf.tanh(tf.squeeze(temp2+self.alpha_bias[i],axis=2)))\n",
    "\n",
    "            #making list of alpha.\n",
    "            alpha=tf.nn.softmax(u,axis=0)\n",
    "\n",
    "            #making L.\n",
    "            L=[]\n",
    "            for i,l in enumerate(new_l):\n",
    "                L.append(alpha[i]*l)\n",
    "            L=tf.reduce_sum(L,axis=0)\n",
    "\n",
    "\n",
    "            #new state = c(t-1) * f + L. new h = tanh(c) + sigmoid(o)\n",
    "            new_c = (c * tf.sigmoid(main_list[0] + self._forget_bias)+L)\n",
    "            new_h = self._activation(new_c) * tf.sigmoid(main_list[1])\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = rnn.LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = tf.concat(1, [new_c, new_h])\n",
    "        return new_h, new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d696a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Layer():\n",
    "    \"\"\"\n",
    "    어텐션 레이어.\n",
    "    (None, TimeWindow, hidden_unit_size) shape의 LSTM 출력을 입력으로 받아 (None, 1, hidden_unit_size)의 텐서 출력.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        timewindow_size,\n",
    "        input_hidden_unit_size,\n",
    "        attention_size=None):\n",
    "        \"\"\"\n",
    "        Setting parameter for attention layer.\n",
    "        args:\n",
    "            timewindow_size = time window size of previous lstm layer.\n",
    "            input_hidden_unit_size = hidden unit number of previous lstm layer.\n",
    "            attention_size = size of this attention. \n",
    "                default = input_hidden_unit_size.\n",
    "        \"\"\"\n",
    "        if(attention_size is None):\n",
    "            attention_size=input_hidden_unit_size\n",
    "        self.o_size=attention_size\n",
    "        self.h_size=input_hidden_unit_size\n",
    "        self.t_size=timewindow_size\n",
    "\n",
    "        self.beta_weight=tf.Variable(tf.random.normal([self.h_size,self.o_size]), name='beta_weight')\n",
    "        self.beta_bias=tf.Variable(tf.zeros([self.o_size]),name='beta_bias')\n",
    "\n",
    "        self.v=tf.Variable(tf.random.normal([self.o_size,1]),name='beta_v')\n",
    "\n",
    "    def __call__(self,inputs):\n",
    "        \"\"\"\n",
    "        producing output with actual inputs.\n",
    "        shape of output will be (batch_size, 1, input_hidden_unit_size).\n",
    "        \"\"\"\n",
    "        #temp = tanh(Y X W + b) ->shape of result = (-1, self.o_size)\n",
    "        temp=tf.matmul(tf.reshape(inputs,[-1,self.h_size]),self.beta_weight)\n",
    "         \n",
    "        temp=tf.tanh(temp+self.beta_bias)\n",
    "        \n",
    "            \n",
    "        #j=temp X v\n",
    "        j=tf.reshape(tf.matmul(temp,self.v),[-1,self.t_size,1])\n",
    "\n",
    "        beta=tf.nn.softmax(j)\n",
    "        \n",
    "        \n",
    "\n",
    "        output=beta*inputs\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff619efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(keras.Model):\n",
    "    \"\"\"\n",
    "    Basic LSTM list for test.\n",
    "    \"\"\"\n",
    "    def __init__(self,windowsize,Pos,Neg):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.T=windowsize\n",
    "        self.P=Pos\n",
    "        self.N=Neg\n",
    "\n",
    "        self.Y=tf.keras.layers.InputLayer(input_shape=(None,self.T,1),dtype = tf.float32)\n",
    "        self.Xp=tf.keras.layers.InputLayer(input_shape=(None,self.P,self.T,1),dtype = tf.float32)\n",
    "        self.Xn=tf.keras.layers.InputLayer(input_shape=(None,self.N,self.T,1),dtype = tf.float32)\n",
    "        self.Xi=tf.keras.layers.InputLayer(input_shape=(None,9,self.T,1),dtype = tf.float32)\n",
    "\n",
    "        self.LSTM1=tf.keras.layers.RNN(tf.keras.layers.LSTMCell(64,name='lstm1'),\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        \n",
    "        #MI-LSTM\n",
    "        self.LSTM2=tf.keras.layers.RNN(MI_LSTMCell(64,4,name='lstm2'),\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        \n",
    "        #Attention_Layer\n",
    "        self.attention_layer=Attention_Layer(self.T,64)\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.dense_layers = tf.keras.models.Sequential()\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    def call(self,y,xp,xn,xi):\n",
    "        Y_1,_,_=self.LSTM1(self.Y(y))\n",
    "        \n",
    "        Xis=tf.split(self.Xi(xi),9,1)\n",
    "        Xi_list=[]\n",
    "        for i in range(len(Xis)):\n",
    "            o,_,_=self.LSTM1(tf.squeeze(Xis[i],axis=1))\n",
    "            Xi_list.append(o)\n",
    "\n",
    "        Xps=tf.split(self.Xp(xp),self.P,1)\n",
    "        Xp_list=[]\n",
    "        for i in range(len(Xps)):\n",
    "            o,_,_=self.LSTM1(tf.squeeze(Xps[i],axis=1))\n",
    "            Xp_list.append(o)\n",
    "        \n",
    "        Xns=tf.split(self.Xn(xn),self.N,1)\n",
    "        Xn_list=[]\n",
    "        for i in range(len(Xns)):\n",
    "            o,_,_=self.LSTM1(tf.squeeze(Xns[i],axis=1))\n",
    "            Xn_list.append(o)\n",
    "        Xp_1=tf.reduce_mean(Xp_list,0)\n",
    "        Xn_1=tf.reduce_mean(Xn_list,0)\n",
    "        Xi_1=tf.reduce_mean(Xi_list,0)\n",
    "\n",
    "        result=tf.concat([Y_1,Xp_1,Xn_1,Xi_1],axis=2)\n",
    "       \n",
    "        Y_2,_,_ =self.LSTM2(result)\n",
    "        \n",
    "        Y_3=self.attention_layer(Y_2)\n",
    "\n",
    "        #Non-linear units for producing final prediction.\n",
    "        R_1 = self.flatten(Y_3)\n",
    "        R_6=self.dense_layers(R_1)\n",
    "\n",
    "        return R_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7016e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_model_iter(Window_size,Window_size_CC):\n",
    "    all_stock = StockData_3(path_asset,\n",
    "                            INDEX_DATA,\n",
    "                            Window_size,\n",
    "                            Window_size_CC,\n",
    "                            positive_correlation_stock_num,\n",
    "                            negative_correlation_stock_num,\n",
    "                            train_test_rate,\n",
    "                            batch_size,\n",
    "                            date_duration = 2000)\n",
    "    \n",
    "    scaler_info_csv(all_stock.scaler, all_stock.indexScaler, Window_size, all_stock.include_asset, all_stock.include_index)\n",
    "    \n",
    "    lstmModel=LSTM_Model(Window_size,\n",
    "                         positive_correlation_stock_num,\n",
    "                         negative_correlation_stock_num)\n",
    "        \n",
    "    result_dic={}\n",
    "        \n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    optimizer=tf.keras.optimizers.Adam(adam_lr)\n",
    "    evalution_costplt=[]\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        #epoch start\n",
    "        start_time = time.time()\n",
    "        training_cost=0\n",
    "        evalution_cost=0\n",
    "    \n",
    "        #training batch\n",
    "        opt = 'training'\n",
    "        for batch in tqdm(all_stock.getBatch('training'),unit=\" Batches\",desc=f\"{opt:>15}\"):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = lstmModel(batch['y'],batch['xp'],batch['xn'],batch['xi'])\n",
    "                loss_value = loss_fn(batch['target'], logits)\n",
    "                loss_value += sum(lstmModel.losses)\n",
    "            training_cost+=loss_value\n",
    "            grads = tape.gradient(loss_value, lstmModel.trainable_weights,unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "            optimizer.apply_gradients(zip(grads, lstmModel.trainable_weights))\n",
    "\n",
    "        #evaluation batch\n",
    "        opt = 'evaluation'\n",
    "        for batch in tqdm(all_stock.getBatch('evaluation'),unit=\" Batches\",desc=f\"{opt:>15}\"):\n",
    "            logits = lstmModel(batch['y'],batch['xp'],batch['xn'],batch['xi'])\n",
    "            loss_value = loss_fn(batch['target'], logits)\n",
    "            loss_value += sum(lstmModel.losses)\n",
    "            evalution_cost+=loss_value\n",
    "   \n",
    "        #epoch end\n",
    "        elapsed_time = time.time()-start_time\n",
    "        training_cost=training_cost/all_stock.batchNum['training']\n",
    "        evalution_cost=evalution_cost/all_stock.batchNum['evaluation']\n",
    "        result_dic[i]=[training_cost,evalution_cost]\n",
    "        evalution_costplt.append(evalution_cost)\n",
    "\n",
    "        print(f'epoch : {i}, t_cost : {training_cost:0.6f}, e_cost : {evalution_cost:0.6f}, elapsed time : {elapsed_time:0.2f}sec')\n",
    "\n",
    "    #Finish epoch\n",
    "    ## save model\n",
    "    os.makedirs(path+\"/model_save\",exist_ok=True)\n",
    "    lstmModel.save(f\"./model_save/windowsize_{Window_size}\")\n",
    "    \n",
    "    sorted_result=sorted(result_dic,key=lambda k:result_dic[k][1])\n",
    "    bestEpoch=sorted_result[0]\n",
    "    print(f'\\n#Best result at epoch {bestEpoch}')\n",
    "    print(f't_cost : {result_dic[bestEpoch][0]:0.6f}, e_cost : {result_dic[bestEpoch][1]:0.6f}')\n",
    "\n",
    "    plt.plot(evalution_costplt)\n",
    "    \n",
    "    os.makedirs(path+\"/evalution_costplt\",exist_ok=True)\n",
    "    plt.savefig(f'./evalution_costplt/windowsize_{Window_size}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69955a",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae168ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in timesize:\n",
    "    t = \" WindowSize \"+str(w)+\" \"\n",
    "    print(f\"{t:=^40}\")\n",
    "    one_model_iter(w,timesize_for_calc_correlation)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "070b7937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockPrice shape:  (2000, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "making dataset progress: 100%|█| 1930/1930 [03:37<\n"
     ]
    }
   ],
   "source": [
    "all_stock = StockData_3(path_asset,\n",
    "                            INDEX_DATA,\n",
    "                            20,\n",
    "                            70,\n",
    "                            30,\n",
    "                            30,\n",
    "                            0.8,\n",
    "                            512,\n",
    "                            date_duration = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8218a271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026046</td>\n",
       "      <td>0.035511</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>0.896919</td>\n",
       "      <td>0.658639</td>\n",
       "      <td>0.956944</td>\n",
       "      <td>0.872475</td>\n",
       "      <td>0.776589</td>\n",
       "      <td>0.126309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028893</td>\n",
       "      <td>0.036974</td>\n",
       "      <td>0.013332</td>\n",
       "      <td>0.907891</td>\n",
       "      <td>0.669831</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.828239</td>\n",
       "      <td>0.793578</td>\n",
       "      <td>0.110401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027256</td>\n",
       "      <td>0.036771</td>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.894626</td>\n",
       "      <td>0.657540</td>\n",
       "      <td>0.952672</td>\n",
       "      <td>0.828739</td>\n",
       "      <td>0.777438</td>\n",
       "      <td>0.110809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026046</td>\n",
       "      <td>0.036486</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>0.894750</td>\n",
       "      <td>0.651744</td>\n",
       "      <td>0.917909</td>\n",
       "      <td>0.790302</td>\n",
       "      <td>0.779986</td>\n",
       "      <td>0.121142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.027669</td>\n",
       "      <td>0.009525</td>\n",
       "      <td>0.882229</td>\n",
       "      <td>0.620166</td>\n",
       "      <td>0.879796</td>\n",
       "      <td>0.801450</td>\n",
       "      <td>0.759599</td>\n",
       "      <td>0.159483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>0.769641</td>\n",
       "      <td>0.813140</td>\n",
       "      <td>0.671902</td>\n",
       "      <td>0.917126</td>\n",
       "      <td>0.794844</td>\n",
       "      <td>0.424066</td>\n",
       "      <td>0.100511</td>\n",
       "      <td>0.894665</td>\n",
       "      <td>0.261455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>0.792129</td>\n",
       "      <td>0.834430</td>\n",
       "      <td>0.695768</td>\n",
       "      <td>0.940433</td>\n",
       "      <td>0.796043</td>\n",
       "      <td>0.427459</td>\n",
       "      <td>0.086457</td>\n",
       "      <td>0.915817</td>\n",
       "      <td>0.249626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>0.820595</td>\n",
       "      <td>0.857102</td>\n",
       "      <td>0.723991</td>\n",
       "      <td>0.946507</td>\n",
       "      <td>0.799940</td>\n",
       "      <td>0.432443</td>\n",
       "      <td>0.099505</td>\n",
       "      <td>0.928984</td>\n",
       "      <td>0.225425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>0.820595</td>\n",
       "      <td>0.857102</td>\n",
       "      <td>0.723991</td>\n",
       "      <td>0.946507</td>\n",
       "      <td>0.799940</td>\n",
       "      <td>0.432443</td>\n",
       "      <td>0.099084</td>\n",
       "      <td>0.928984</td>\n",
       "      <td>0.225425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>0.815969</td>\n",
       "      <td>0.850723</td>\n",
       "      <td>0.723530</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.803737</td>\n",
       "      <td>0.423438</td>\n",
       "      <td>0.093387</td>\n",
       "      <td>0.928984</td>\n",
       "      <td>0.252345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2675 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.026046  0.035511  0.011898  0.896919  0.658639  0.956944  0.872475   \n",
       "1     0.028893  0.036974  0.013332  0.907891  0.669831  1.000000  0.828239   \n",
       "2     0.027256  0.036771  0.013350  0.894626  0.657540  0.952672  0.828739   \n",
       "3     0.026046  0.036486  0.011402  0.894750  0.651744  0.917909  0.790302   \n",
       "4     0.019641  0.027669  0.009525  0.882229  0.620166  0.879796  0.801450   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2670  0.769641  0.813140  0.671902  0.917126  0.794844  0.424066  0.100511   \n",
       "2671  0.792129  0.834430  0.695768  0.940433  0.796043  0.427459  0.086457   \n",
       "2672  0.820595  0.857102  0.723991  0.946507  0.799940  0.432443  0.099505   \n",
       "2673  0.820595  0.857102  0.723991  0.946507  0.799940  0.432443  0.099084   \n",
       "2674  0.815969  0.850723  0.723530  0.971549  0.803737  0.423438  0.093387   \n",
       "\n",
       "             7         8  \n",
       "0     0.776589  0.126309  \n",
       "1     0.793578  0.110401  \n",
       "2     0.777438  0.110809  \n",
       "3     0.779986  0.121142  \n",
       "4     0.759599  0.159483  \n",
       "...        ...       ...  \n",
       "2670  0.894665  0.261455  \n",
       "2671  0.915817  0.249626  \n",
       "2672  0.928984  0.225425  \n",
       "2673  0.928984  0.225425  \n",
       "2674  0.928984  0.252345  \n",
       "\n",
       "[2675 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler =  MinMaxScaler()\n",
    "\n",
    "I = INDEX_DATA.drop(columns = \"Date\").copy()\n",
    "\n",
    "scaler.fit(I)\n",
    "pd.DataFrame(scaler.transform(I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcfd27b9-f90d-443f-af90-97a3b3573238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SP500',\n",
       " 'Dow',\n",
       " 'Nasdaq',\n",
       " 'CrudeOil',\n",
       " 'Gold',\n",
       " 'Silver',\n",
       " 'EURUSD',\n",
       " 'gsci',\n",
       " 'vix']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "553ff682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_info_csv(asset_scaler, index_scaler, Window_size, asset_list, index_list):\n",
    "    \"\"\"\n",
    "    how to use? : (X * scale_) + min_ = scaled_X\n",
    "    \"\"\"\n",
    "    asset_scaler_info = pd.DataFrame([asset_scaler.scale_,asset_scaler.min_]).T\n",
    "    asset_scaler_info.columns = [\"scale_\",\"min_\"]\n",
    "    asset_scaler_info.index = asset_list\n",
    "    \n",
    "    index_scaler_info = pd.DataFrame([index_scaler.scale_,index_scaler.min_]).T\n",
    "    index_scaler_info.columns = [\"scale_\",\"min_\"]\n",
    "    index_scaler_info.index = index_list\n",
    "    \n",
    "    os.makedirs(f\"./scaler_info/window_{Window_size}\",exist_ok = True)\n",
    "    \n",
    "    asset_scaler_info.to_csv(f\"./scaler_info/window_{Window_size}/asset_scaler.csv\",index=False)\n",
    "    index_scaler_info.to_csv(f\"./scaler_info/window_{Window_size}/index_scaler.csv\",index=False)\n",
    "    \n",
    "    return asset_scaler_info,index_scaler_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d96037aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_s_i,I_s_i = scaler_info_csv(all_stock.scaler, all_stock.indexScaler, 20, all_stock.include_asset, all_stock.include_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63b0c818-2b96-486b-ac6a-35396453af89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scale_</th>\n",
       "      <th>min_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>0.000569</td>\n",
       "      <td>-1.724737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dow</th>\n",
       "      <td>0.000081</td>\n",
       "      <td>-1.980254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nasdaq</th>\n",
       "      <td>0.000142</td>\n",
       "      <td>-1.347734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CrudeOil</th>\n",
       "      <td>0.012397</td>\n",
       "      <td>-0.533503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gold</th>\n",
       "      <td>0.001999</td>\n",
       "      <td>-3.100130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Silver</th>\n",
       "      <td>0.083766</td>\n",
       "      <td>-1.982995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EURUSD</th>\n",
       "      <td>5.631751</td>\n",
       "      <td>-6.845521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsci</th>\n",
       "      <td>0.003398</td>\n",
       "      <td>-1.835542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vix</th>\n",
       "      <td>0.027192</td>\n",
       "      <td>-1.248538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            scale_      min_\n",
       "SP500     0.000569 -1.724737\n",
       "Dow       0.000081 -1.980254\n",
       "Nasdaq    0.000142 -1.347734\n",
       "CrudeOil  0.012397 -0.533503\n",
       "Gold      0.001999 -3.100130\n",
       "Silver    0.083766 -1.982995\n",
       "EURUSD    5.631751 -6.845521\n",
       "gsci      0.003398 -1.835542\n",
       "vix       0.027192 -1.248538"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_s_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e0f5095-b041-423c-8502-160f262a9fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scale_</th>\n",
       "      <th>min_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABBV</th>\n",
       "      <td>0.014573</td>\n",
       "      <td>-1.528193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>0.005767</td>\n",
       "      <td>-1.382248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AEP</th>\n",
       "      <td>0.030307</td>\n",
       "      <td>-2.141966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIZ</th>\n",
       "      <td>0.014183</td>\n",
       "      <td>-1.728744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLE</th>\n",
       "      <td>0.019082</td>\n",
       "      <td>-1.801322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLY</th>\n",
       "      <td>0.013051</td>\n",
       "      <td>-1.750794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLI</th>\n",
       "      <td>0.031247</td>\n",
       "      <td>-2.333929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLU</th>\n",
       "      <td>0.043548</td>\n",
       "      <td>-2.351434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLP</th>\n",
       "      <td>0.044041</td>\n",
       "      <td>-2.548361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLB</th>\n",
       "      <td>0.034746</td>\n",
       "      <td>-2.146241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scale_      min_\n",
       "ABBV  0.014573 -1.528193\n",
       "ACN   0.005767 -1.382248\n",
       "AEP   0.030307 -2.141966\n",
       "AIZ   0.014183 -1.728744\n",
       "ALLE  0.019082 -1.801322\n",
       "...        ...       ...\n",
       "XLY   0.013051 -1.750794\n",
       "XLI   0.031247 -2.333929\n",
       "XLU   0.043548 -2.351434\n",
       "XLP   0.044041 -2.548361\n",
       "XLB   0.034746 -2.146241\n",
       "\n",
       "[84 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_s_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db89e82-61bf-453f-a3ee-d16b7090300c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M6_LJY",
   "language": "python",
   "name": "m6_ljy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
